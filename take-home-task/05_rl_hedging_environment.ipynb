{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "073b49c6",
   "metadata": {},
   "source": [
    "### 05 — RL Hedging Environment\n",
    "\n",
    "In this notebook you turn the components from Notebooks 02–04 into a **Markov Decision Process (MDP)** environment for an **RL-based option hedging policy**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.1 MDP Structure (Fixed Components)\n",
    "\n",
    "We model the hedging problem as a finite-horizon MDP\n",
    "$$\n",
    "\\mathcal{M} = (S, A, P, R, \\gamma).\n",
    "$$\n",
    "\n",
    "- **State** $s_n$: contains market, MM and option risk information at decision time $t_n$.\n",
    "- **Action** $a_n$: choose a **target net delta / net vega bucket** for the overall portfolio, which is then implemented via option trades in a fixed option universe.\n",
    "- **Transition** $P$: driven by the BTC QED+Hawkes simulator and the MM strategy.\n",
    "- **Reward** $r_n$: change in portfolio equity, penalised by option costs and risk measures.\n",
    "- **Discount** $\\gamma$: you may use $\\gamma=1$ for episodic training on finite horizons.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce9fcab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hedging Environment Test\n",
      "Initial state shape: (9,)\n",
      "Action space size: 25\n",
      "Initial total equity: 100000.00\n",
      "Initial portfolio delta: 0.00\n",
      "Initial portfolio vega: 0.00\n",
      "\n",
      "Testing environment steps:\n",
      "Step 1: Action=sell_0.9_7_call, Reward=0.00, Equity=100000.00\n",
      "Step 2: Action=buy_0.9_7_put, Reward=-0.13, Equity=99999.99\n",
      "Step 3: Action=buy_1.1_7_call, Reward=23.75, Equity=100023.78\n",
      "\n",
      "Final portfolio delta: 0.00\n",
      "Final portfolio vega: 133.09\n",
      "Final total equity: 100023.78\n"
     ]
    }
   ],
   "source": [
    "# write your code and analysis here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from enum import Enum\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seeds(42)\n",
    "\n",
    "# 5.1 MDP Structure\n",
    "class HedgingEnvironment:\n",
    "    def __init__(self):\n",
    "        self.strikes = [0.9, 1.0, 1.1]  # Moneyness ratios\n",
    "        self.maturities = [1, 7]  # Days\n",
    "        self.option_types = ['call', 'put']\n",
    "        self.lot_size = 0.1  # BTC per option contract\n",
    "        self.transaction_cost = 0.0005  # 0.05%\n",
    "        \n",
    "        # Initialize state\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to initial state\"\"\"\n",
    "        # Market state\n",
    "        self.S = 50000  # BTC price\n",
    "        self.local_vol = 0.6  # Local volatility\n",
    "        self.time_step = 0\n",
    "        self.max_steps = 100\n",
    "        \n",
    "        # MM state\n",
    "        self.I = 0.0  # Inventory\n",
    "        self.cash = 100000  # Cash\n",
    "        self.mm_equity = self.cash + self.I * self.S\n",
    "        \n",
    "        # Option portfolio\n",
    "        self.option_positions = self._initialize_option_positions()\n",
    "        self.option_prices = self._compute_option_prices()\n",
    "        \n",
    "        # Greeks\n",
    "        self.portfolio_delta = self.I  # MM delta = inventory\n",
    "        self.portfolio_vega = 0.0\n",
    "        \n",
    "        # History\n",
    "        self.equity_history = [self.total_equity]\n",
    "        self.delta_history = [self.portfolio_delta]\n",
    "        self.vega_history = [self.portfolio_vega]\n",
    "        \n",
    "        return self.state\n",
    "    \n",
    "    @property\n",
    "    def total_equity(self):\n",
    "        \"\"\"Total equity: MM + options\"\"\"\n",
    "        option_value = sum(pos * price for pos, price in \n",
    "                          zip(self.option_positions.values(), self.option_prices.values()))\n",
    "        return self.mm_equity + option_value\n",
    "    \n",
    "    @property\n",
    "    def state(self):\n",
    "        \"\"\"State representation for RL agent\"\"\"\n",
    "        return np.array([\n",
    "            self.S / 50000,  # Normalized price\n",
    "            self.I / 10,  # Normalized inventory\n",
    "            self.total_equity / 100000,  # Normalized equity\n",
    "            self.local_vol,\n",
    "            self.portfolio_delta / 10,  # Normalized delta\n",
    "            self.portfolio_vega / 1000,  # Normalized vega\n",
    "            self.time_step / self.max_steps,  # Progress\n",
    "            np.random.normal(0, 0.01),  # Price change (simplified)\n",
    "            np.random.normal(0, 0.001)  # Vol change (simplified)\n",
    "        ])\n",
    "    \n",
    "    def _initialize_option_positions(self):\n",
    "        \"\"\"Initialize option positions to zero\"\"\"\n",
    "        positions = {}\n",
    "        for strike in self.strikes:\n",
    "            for maturity in self.maturities:\n",
    "                for opt_type in self.option_types:\n",
    "                    key = f\"{strike}_{maturity}_{opt_type}\"\n",
    "                    positions[key] = 0\n",
    "        return positions\n",
    "    \n",
    "    def _compute_option_prices(self):\n",
    "        \"\"\"Compute option prices using Black-Scholes\"\"\"\n",
    "        prices = {}\n",
    "        for strike_ratio in self.strikes:\n",
    "            for maturity in self.maturities:\n",
    "                for opt_type in self.option_types:\n",
    "                    K = strike_ratio * 50000  # Strike price\n",
    "                    tau = maturity / 365  # Time to maturity\n",
    "                    \n",
    "                    # Simplified Black-Scholes\n",
    "                    price = self._black_scholes(self.S, K, tau, self.local_vol, opt_type)\n",
    "                    key = f\"{strike_ratio}_{maturity}_{opt_type}\"\n",
    "                    prices[key] = price\n",
    "        return prices\n",
    "    \n",
    "    def _black_scholes(self, S, K, tau, sigma, option_type):\n",
    "        \"\"\"Simplified Black-Scholes pricing\"\"\"\n",
    "        if tau <= 0:\n",
    "            if option_type == 'call':\n",
    "                return max(S - K, 0)\n",
    "            else:\n",
    "                return max(K - S, 0)\n",
    "        \n",
    "        d1 = (np.log(S/K) + 0.5 * sigma**2 * tau) / (sigma * np.sqrt(tau))\n",
    "        d2 = d1 - sigma * np.sqrt(tau)\n",
    "        \n",
    "        if option_type == 'call':\n",
    "            price = S * self._norm_cdf(d1) - K * self._norm_cdf(d2)\n",
    "        else:\n",
    "            price = K * self._norm_cdf(-d2) - S * self._norm_cdf(-d1)\n",
    "        \n",
    "        return max(price, 0)\n",
    "    \n",
    "    def _norm_cdf(self, x):\n",
    "        \"\"\"Normal CDF approximation\"\"\"\n",
    "        return 0.5 * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))\n",
    "    \n",
    "    def _compute_greeks(self):\n",
    "        \"\"\"Compute portfolio Greeks\"\"\"\n",
    "        delta_total = self.I  # Start with MM delta\n",
    "        vega_total = 0.0\n",
    "        \n",
    "        for key, position in self.option_positions.items():\n",
    "            # Parse key to get strike, maturity, type\n",
    "            parts = key.split('_')\n",
    "            strike_ratio = float(parts[0])\n",
    "            maturity = int(parts[1])\n",
    "            opt_type = parts[2]\n",
    "            \n",
    "            K = strike_ratio * 50000\n",
    "            tau = maturity / 365\n",
    "            \n",
    "            # Simplified delta and vega\n",
    "            if opt_type == 'call':\n",
    "                delta = 0.6 if self.S > K else 0.4  # Simplified\n",
    "            else:\n",
    "                delta = -0.4 if self.S > K else -0.6  # Simplified\n",
    "            \n",
    "            vega = 0.1 * self.S * np.sqrt(tau)  # Simplified vega\n",
    "            \n",
    "            delta_total += position * delta\n",
    "            vega_total += position * vega\n",
    "        \n",
    "        self.portfolio_delta = delta_total\n",
    "        self.portfolio_vega = vega_total\n",
    "    \n",
    "    # 5.5 Action Space\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        \"\"\"Discrete action space: no trade + buy/sell for each option\"\"\"\n",
    "        actions = ['no_trade']\n",
    "        \n",
    "        # Add buy/sell actions for each option\n",
    "        for strike in self.strikes:\n",
    "            for maturity in self.maturities:\n",
    "                for opt_type in self.option_types:\n",
    "                    actions.append(f\"buy_{strike}_{maturity}_{opt_type}\")\n",
    "                    actions.append(f\"sell_{strike}_{maturity}_{opt_type}\")\n",
    "        \n",
    "        return actions\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Execute one environment step\"\"\"\n",
    "        # Store previous equity for reward calculation\n",
    "        prev_equity = self.total_equity\n",
    "        \n",
    "        # Update market state (simplified)\n",
    "        self._update_market_state()\n",
    "        \n",
    "        # Execute action\n",
    "        transaction_cost = self._execute_action(action)\n",
    "        \n",
    "        # Update Greeks\n",
    "        self._compute_greeks()\n",
    "        \n",
    "        # Update MM equity\n",
    "        self.mm_equity = self.cash + self.I * self.S\n",
    "        \n",
    "        # Compute reward\n",
    "        reward = self._compute_reward(prev_equity, transaction_cost)\n",
    "        \n",
    "        # Update history\n",
    "        self.time_step += 1\n",
    "        self.equity_history.append(self.total_equity)\n",
    "        self.delta_history.append(self.portfolio_delta)\n",
    "        self.vega_history.append(self.portfolio_vega)\n",
    "        \n",
    "        # Check termination\n",
    "        done = self.time_step >= self.max_steps\n",
    "        \n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "    def _update_market_state(self):\n",
    "        \"\"\"Update market state (simplified)\"\"\"\n",
    "        # Random price movement\n",
    "        price_change = np.random.normal(0, 0.01) * self.S\n",
    "        self.S += price_change\n",
    "        \n",
    "        # Random volatility change\n",
    "        vol_change = np.random.normal(0, 0.05)\n",
    "        self.local_vol = max(0.3, min(1.0, self.local_vol + vol_change))\n",
    "        \n",
    "        # Update option prices\n",
    "        self.option_prices = self._compute_option_prices()\n",
    "    \n",
    "    def _execute_action(self, action):\n",
    "        \"\"\"Execute trading action and return transaction cost\"\"\"\n",
    "        if action == 'no_trade':\n",
    "            return 0\n",
    "        \n",
    "        # Parse action\n",
    "        parts = action.split('_')\n",
    "        action_type, strike_ratio, maturity, opt_type = parts[0], float(parts[1]), int(parts[2]), parts[3]\n",
    "        key = f\"{strike_ratio}_{maturity}_{opt_type}\"\n",
    "        \n",
    "        # Get option price\n",
    "        option_price = self.option_prices[key]\n",
    "        \n",
    "        # Determine trade direction and size\n",
    "        if action_type == 'buy':\n",
    "            trade_size = self.lot_size\n",
    "            self.option_positions[key] += trade_size\n",
    "            self.cash -= trade_size * option_price\n",
    "        else:  # sell\n",
    "            trade_size = self.lot_size\n",
    "            # Check if we have enough to sell\n",
    "            if self.option_positions[key] >= trade_size:\n",
    "                self.option_positions[key] -= trade_size\n",
    "                self.cash += trade_size * option_price\n",
    "            else:\n",
    "                # If not enough, sell what we have\n",
    "                trade_size = self.option_positions[key]\n",
    "                self.option_positions[key] = 0\n",
    "                self.cash += trade_size * option_price\n",
    "        \n",
    "        # Compute transaction cost\n",
    "        notional = trade_size * option_price\n",
    "        transaction_cost = self.transaction_cost * notional\n",
    "        self.cash -= transaction_cost\n",
    "        \n",
    "        return transaction_cost\n",
    "    \n",
    "    # 5.7 Reward Design\n",
    "    def _compute_reward(self, prev_equity, transaction_cost):\n",
    "        \"\"\"\n",
    "        Reward function design:\n",
    "        r = PnL - cost_penalty - risk_penalty - tail_risk_penalty\n",
    "        \"\"\"\n",
    "        # 1. Profit focus: Realized PnL\n",
    "        pnl = self.total_equity - prev_equity\n",
    "        \n",
    "        # 2. Cost awareness: Transaction cost penalty\n",
    "        cost_penalty = transaction_cost * 10  # Scale for balance\n",
    "        \n",
    "        # 3. Risk exposure control: Soft penalties for large exposures\n",
    "        delta_penalty = 0.01 * max(0, abs(self.portfolio_delta) - 5) ** 2\n",
    "        vega_penalty = 0.001 * max(0, abs(self.portfolio_vega) - 500) ** 2\n",
    "        \n",
    "        # 4. Tail-risk awareness: Drawdown penalty\n",
    "        current_drawdown = 0\n",
    "        if len(self.equity_history) > 0:\n",
    "            peak_equity = max(self.equity_history)\n",
    "            if peak_equity > 0:\n",
    "                current_drawdown = (peak_equity - self.total_equity) / peak_equity\n",
    "        drawdown_penalty = 100 * max(0, current_drawdown - 0.1) ** 2\n",
    "        \n",
    "        # 5. Final outcome penalty for large losses\n",
    "        final_penalty = 0\n",
    "        if self.time_step == self.max_steps - 1:  # Last step\n",
    "            if self.total_equity < 90000:  # Large loss threshold\n",
    "                final_penalty = 50 * (100000 - self.total_equity) / 10000\n",
    "        \n",
    "        reward = pnl - cost_penalty - delta_penalty - vega_penalty - drawdown_penalty - final_penalty\n",
    "        \n",
    "        return reward\n",
    "\n",
    "# Test the environment\n",
    "if __name__ == \"__main__\":\n",
    "    env = HedgingEnvironment()\n",
    "    \n",
    "    print(\"Hedging Environment Test\")\n",
    "    print(f\"Initial state shape: {env.state.shape}\")\n",
    "    print(f\"Action space size: {len(env.action_space)}\")\n",
    "    print(f\"Initial total equity: {env.total_equity:.2f}\")\n",
    "    print(f\"Initial portfolio delta: {env.portfolio_delta:.2f}\")\n",
    "    print(f\"Initial portfolio vega: {env.portfolio_vega:.2f}\")\n",
    "    \n",
    "    # Test a few steps\n",
    "    print(\"\\nTesting environment steps:\")\n",
    "    for i in range(3):\n",
    "        action = np.random.choice(env.action_space)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        print(f\"Step {i+1}: Action={action}, Reward={reward:.2f}, Equity={env.total_equity:.2f}\")\n",
    "    \n",
    "    print(f\"\\nFinal portfolio delta: {env.portfolio_delta:.2f}\")\n",
    "    print(f\"Final portfolio vega: {env.portfolio_vega:.2f}\")\n",
    "    print(f\"Final total equity: {env.total_equity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ec842c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 5.2 Hedge Universe: Strikes and Maturities\n",
    "\n",
    "The RL agent trades only within a **small, liquid universe** of BTC options:\n",
    "\n",
    "* **Moneyness / strikes**:\n",
    "  $$\n",
    "  K \\in {0.9 S_0,; 1.0 S_0,; 1.1 S_0},\n",
    "  $$\n",
    "  corresponding to 10% OTM, ATM, and 10% OTM on the other side.\n",
    "\n",
    "* **Maturities**:\n",
    "  $$\n",
    "  T \\in {1\\text{d},; 7\\text{d}}.\n",
    "  $$\n",
    "\n",
    "* **Types**:\n",
    "\n",
    "  * Calls and puts on the BTC simulator price $S_n$.\n",
    "\n",
    "This gives a natural universe of up to:\n",
    "\n",
    "* $3$ strikes $\\times$ $2$ maturities $\\times$ $2$ types (call/put)\n",
    "  $= 12$ distinct option contracts.\n",
    "\n",
    "You may restrict to a smaller subset (e.g. ATM options only) for computational reasons, but the default assumption is that the agent **has access to all** of these contracts.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.3 Portfolio, Delta and Vega\n",
    "\n",
    "Let:\n",
    "\n",
    "* $I_n$: MM inventory in the BTC perpetual at time $n$.\n",
    "* $Q_n^{(i)}$: position (in lots) in option contract $i$ at time $n$\n",
    "  (e.g. “number of contracts” or a normalised lot size).\n",
    "* $P_n^{(i)}$: price of option $i$ at time $n$.\n",
    "\n",
    "The **total equity** (MM + options) is:\n",
    "\n",
    "$$\n",
    "\\Pi_n^{\\text{total}}\n",
    "= \\Pi_n^{\\text{MM}} + \\sum_i Q_n^{(i)} P_n^{(i)},\n",
    "$$\n",
    "\n",
    "where $\\Pi_n^{\\text{MM}}$ is the equity of the MM engine alone.\n",
    "\n",
    "We define:\n",
    "\n",
    "* $\\Delta^{\\text{MM}}_n$: delta of MM position (essentially $I_n$ if perp is 1:1 delta).\n",
    "* $\\Delta^{(i)}_n$: delta of option $i$ at time $n$.\n",
    "* $\\Delta^{\\text{opt}}_n = \\sum_i Q_n^{(i)} \\Delta^{(i)}_n$: aggregate option delta.\n",
    "* $\\Delta^{\\text{port}}_n = \\Delta^{\\text{MM}}_n + \\Delta^{\\text{opt}}_n$: net portfolio delta.\n",
    "\n",
    "Similarly for vega:\n",
    "\n",
    "* $V^{(i)}_n$: vega of option $i$ at time $n$.\n",
    "* $V^{\\text{opt}}_n = \\sum_i Q_n^{(i)} V^{(i)}_n$.\n",
    "* $V^{\\text{port}}_n = V^{\\text{opt}}_n$ (perpetual has negligible vega).\n",
    "\n",
    "These quantities are part of the **risk state** the RL agent must learn to control.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.4 State Representation\n",
    "\n",
    "At each hedge decision time $n$ (e.g. every few 5-minute steps), the agent observes a state vector $s_n$.\n",
    "A reasonable baseline state includes:\n",
    "\n",
    "$$\n",
    "s_n = (\n",
    "S_n,;\n",
    "I_n,;\n",
    "\\Pi_n^{\\text{total}},;\n",
    "\\hat{\\sigma}_{\\text{loc}}(n),;\n",
    "\\Delta^{\\text{port}}_n,;\n",
    "V^{\\text{port}}_n,;\n",
    "\\text{TTM features},;\n",
    "\\text{moneyness features},;\n",
    "\\Delta S_n,;\n",
    "\\Delta V_n\n",
    ").\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $S_n$: BTC price.\n",
    "* $I_n$: MM inventory (BTC).\n",
    "* $\\Pi_n^{\\text{total}}$: current equity of MM + options.\n",
    "* $\\hat{\\sigma}_{\\text{loc}}(n)$: local realised volatility from Section 3.\n",
    "* $\\Delta^{\\text{port}}_n$: net portfolio delta.\n",
    "* $V^{\\text{port}}_n$: net portfolio vega.\n",
    "* TTM features: time to maturity of relevant contracts (e.g. normalised).\n",
    "* Moneyness features: e.g. $\\log(K/S_n)$ for representative strikes.\n",
    "* $\\Delta S_n$: recent price change(s).\n",
    "* $\\Delta V_n$: recent changes in option prices or IV.\n",
    "\n",
    "You may add/remove features (e.g. regime indicators, jump flags, realised variance windows), as long as you justify your design choices.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.5 Action Space: Discrete Option Trades\n",
    "\n",
    "At each decision time, the agent chooses **one discrete action** from a finite set $A$.\n",
    "\n",
    "The natural design, given our universe, is:\n",
    "\n",
    "* **No trade:**\n",
    "\n",
    "  * Do nothing this step.\n",
    "\n",
    "* **Option trades:**\n",
    "\n",
    "  * For each option contract $i$ in the universe\n",
    "    (strike $K \\in {0.9 S_0, 1.0 S_0, 1.1 S_0}$,\n",
    "    maturity $T \\in {1\\text{d}, 7\\text{d}}$,\n",
    "    call or put), define:\n",
    "\n",
    "    * “buy 1 lot of option $i$”,\n",
    "    * “sell 1 lot of option $i$”.\n",
    "\n",
    "Let $q_{\\text{opt}}$ be the **fixed lot size** per trade.\n",
    "Then a “buy” action increases $Q_n^{(i)}$ by $+q_{\\text{opt}}$,\n",
    "and a “sell” action decreases $Q_n^{(i)}$ by $-q_{\\text{opt}}$.\n",
    "\n",
    "You may optionally restrict the action space to a smaller subset of contracts\n",
    "(e.g. ATM 1d and ATM 7d only) if needed.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.6 Transaction Costs (Size-Aware)\n",
    "\n",
    "Each option trade incurs a transaction cost **proportional to notional**.\n",
    "If at time $n$ we execute trades $\\Delta Q_n^{(i)}$ in each contract $i$, then:\n",
    "\n",
    "$$\n",
    "TC_n = c_{\\text{opt}} \\sum_i \\left| \\Delta Q_n^{(i)} P_n^{(i)} \\right|,\n",
    "$$\n",
    "\n",
    "where $c_{\\text{opt}}$ is a cost rate (e.g. $0.0005$ for $0.05%$).\n",
    "\n",
    "This cost term is **size-aware**:\n",
    "\n",
    "* larger lots or more expensive options\n",
    "  $\\Rightarrow$ larger notional\n",
    "  $\\Rightarrow$ larger $TC_n$ penalty.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.7 Reward design\n",
    "\n",
    "design and justify your own RL reward function.\n",
    "The RL agent should not be forced to keep the book exactly delta– and vega–neutral.  \n",
    "We want to **allow under-hedging / over-hedging** as long as the **overall portfolio is profitable** and **tail risk is controlled**.\n",
    "\n",
    "Your reward design must satisfy the following principles:\n",
    "\n",
    "* **Profit focus.**\n",
    "  The main positive signal should be **realised PnL**, net of transaction costs for hedging trades.\n",
    "  Make clear what PnL you are using (per-step or cumulative increment).\n",
    "\n",
    "* **Cost awareness.**\n",
    "  Transaction costs for option hedges must enter the reward with the correct sign\n",
    "  (higher costs should reduce reward).\n",
    "\n",
    "* **Risk exposure control, but not hard neutrality.**\n",
    "  You may expose the book to delta and vega risk, and the agent is allowed to under–hedge or over–hedge.\n",
    "  However, your reward should **discourage extremely large risk exposures**\n",
    "  (for example via soft penalties once $|\\Delta^{\\text{port}}_n|$ or $|V^{\\text{port}}_n|$ exceed some comfort band).\n",
    "\n",
    "* **Tail–risk awareness.**\n",
    "  Include at least one component that penalises **bad tail outcomes over the whole episode**,\n",
    "  such as large final loss, large drawdown, or a risk measure like downside variance or CVaR.\n",
    "  This should make “rare but very large losses” unattractive even if average PnL is high.\n",
    "\n",
    "* **No trivial solutions.**\n",
    "  Check that your reward does **not** make degenerate policies obviously optimal\n",
    "  (e.g. “never hedge” or “always fully hedge to zero risk” regardless of market conditions).\n",
    "\n",
    "What you need to hand in:\n",
    "\n",
    "* A **mathematical expression** of your reward (per-step and/or terminal), with all symbols defined.\n",
    "* A short **written justification**  explaining:\n",
    "\n",
    "  * how your reward trades off profit vs risk and transaction costs;\n",
    "  * why it allows meaningful under–hedging / over–hedging;\n",
    "  * why it is suitable for controlling tail risk in this assignment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbade504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code and analysis here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
